{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous_step_size 5.4\n",
      "cur_x 0.5999999999999996\n",
      "gamma 0.01\n",
      "prev_x 6 \n",
      "\n",
      "previous_step_size 0.023760000000000003\n",
      "cur_x 0.6237599999999996\n",
      "gamma 0.01\n",
      "prev_x 0.5999999999999996 \n",
      "\n",
      "previous_step_size 0.025309273140264987\n",
      "cur_x 0.6490692731402646\n",
      "gamma 0.01\n",
      "prev_x 0.6237599999999996 \n",
      "\n",
      "previous_step_size 0.026978303236479206\n",
      "cur_x 0.6760475763767438\n",
      "gamma 0.01\n",
      "prev_x 0.6490692731402646 \n",
      "\n",
      "previous_step_size 0.028774389122137145\n",
      "cur_x 0.704821965498881\n",
      "gamma 0.01\n",
      "prev_x 0.6760475763767438 \n",
      "\n",
      "previous_step_size 0.030704171104943834\n",
      "cur_x 0.7355261366038248\n",
      "gamma 0.01\n",
      "prev_x 0.704821965498881 \n",
      "\n",
      "previous_step_size 0.03277313550751959\n",
      "cur_x 0.7682992721113444\n",
      "gamma 0.01\n",
      "prev_x 0.7355261366038248 \n",
      "\n",
      "previous_step_size 0.03498495575728611\n",
      "cur_x 0.8032842278686305\n",
      "gamma 0.01\n",
      "prev_x 0.7682992721113444 \n",
      "\n",
      "previous_step_size 0.03734063397888843\n",
      "cur_x 0.840624861847519\n",
      "gamma 0.01\n",
      "prev_x 0.8032842278686305 \n",
      "\n",
      "previous_step_size 0.03983740658234747\n",
      "cur_x 0.8804622684298664\n",
      "gamma 0.01\n",
      "prev_x 0.840624861847519 \n",
      "\n",
      "previous_step_size 0.04246738230109215\n",
      "cur_x 0.9229296507309586\n",
      "gamma 0.01\n",
      "prev_x 0.8804622684298664 \n",
      "\n",
      "previous_step_size 0.04521589529960479\n",
      "cur_x 0.9681455460305634\n",
      "gamma 0.01\n",
      "prev_x 0.9229296507309586 \n",
      "\n",
      "previous_step_size 0.04805958449122871\n",
      "cur_x 1.016205130521792\n",
      "gamma 0.01\n",
      "prev_x 0.9681455460305634 \n",
      "\n",
      "previous_step_size 0.05096425942090499\n",
      "cur_x 1.067169389942697\n",
      "gamma 0.01\n",
      "prev_x 1.016205130521792 \n",
      "\n",
      "previous_step_size 0.05388268959034348\n",
      "cur_x 1.1210520795330405\n",
      "gamma 0.01\n",
      "prev_x 1.067169389942697 \n",
      "\n",
      "previous_step_size 0.05675256261424311\n",
      "cur_x 1.1778046421472836\n",
      "gamma 0.01\n",
      "prev_x 1.1210520795330405 \n",
      "\n",
      "previous_step_size 0.0594949956770483\n",
      "cur_x 1.237299637824332\n",
      "gamma 0.01\n",
      "prev_x 1.1778046421472836 \n",
      "\n",
      "previous_step_size 0.06201414040877884\n",
      "cur_x 1.2993137782331108\n",
      "gamma 0.01\n",
      "prev_x 1.237299637824332 \n",
      "\n",
      "previous_step_size 0.06419855881437808\n",
      "cur_x 1.3635123370474889\n",
      "gamma 0.01\n",
      "prev_x 1.2993137782331108 \n",
      "\n",
      "previous_step_size 0.06592510511101723\n",
      "cur_x 1.429437442158506\n",
      "gamma 0.01\n",
      "prev_x 1.3635123370474889 \n",
      "\n",
      "previous_step_size 0.0670659367382691\n",
      "cur_x 1.4965033788967752\n",
      "gamma 0.01\n",
      "prev_x 1.429437442158506 \n",
      "\n",
      "previous_step_size 0.06749890133771519\n",
      "cur_x 1.5640022802344904\n",
      "gamma 0.01\n",
      "prev_x 1.4965033788967752 \n",
      "\n",
      "previous_step_size 0.06712084685040987\n",
      "cur_x 1.6311231270849003\n",
      "gamma 0.01\n",
      "prev_x 1.5640022802344904 \n",
      "\n",
      "previous_step_size 0.06586242786245022\n",
      "cur_x 1.6969855549473505\n",
      "gamma 0.01\n",
      "prev_x 1.6311231270849003 \n",
      "\n",
      "previous_step_size 0.06370195454962091\n",
      "cur_x 1.7606875094969714\n",
      "gamma 0.01\n",
      "prev_x 1.6969855549473505 \n",
      "\n",
      "previous_step_size 0.06067515017798364\n",
      "cur_x 1.821362659674955\n",
      "gamma 0.01\n",
      "prev_x 1.7606875094969714 \n",
      "\n",
      "previous_step_size 0.05687780792099262\n",
      "cur_x 1.8782404675959476\n",
      "gamma 0.01\n",
      "prev_x 1.821362659674955 \n",
      "\n",
      "previous_step_size 0.052459541600431425\n",
      "cur_x 1.930700009196379\n",
      "gamma 0.01\n",
      "prev_x 1.8782404675959476 \n",
      "\n",
      "previous_step_size 0.04760893808460742\n",
      "cur_x 1.9783089472809865\n",
      "gamma 0.01\n",
      "prev_x 1.930700009196379 \n",
      "\n",
      "previous_step_size 0.0425327592882192\n",
      "cur_x 2.0208417065692057\n",
      "gamma 0.01\n",
      "prev_x 1.9783089472809865 \n",
      "\n",
      "previous_step_size 0.03743347657569185\n",
      "cur_x 2.0582751831448975\n",
      "gamma 0.01\n",
      "prev_x 2.0208417065692057 \n",
      "\n",
      "previous_step_size 0.032489662383209605\n",
      "cur_x 2.090764845528107\n",
      "gamma 0.01\n",
      "prev_x 2.0582751831448975 \n",
      "\n",
      "previous_step_size 0.027842570193437943\n",
      "cur_x 2.118607415721545\n",
      "gamma 0.01\n",
      "prev_x 2.090764845528107 \n",
      "\n",
      "previous_step_size 0.02359021082166146\n",
      "cur_x 2.1421976265432066\n",
      "gamma 0.01\n",
      "prev_x 2.118607415721545 \n",
      "\n",
      "previous_step_size 0.019788249686815806\n",
      "cur_x 2.1619858762300224\n",
      "gamma 0.01\n",
      "prev_x 2.1421976265432066 \n",
      "\n",
      "previous_step_size 0.016455764593524602\n",
      "cur_x 2.178441640823547\n",
      "gamma 0.01\n",
      "prev_x 2.1619858762300224 \n",
      "\n",
      "previous_step_size 0.013583516820820485\n",
      "cur_x 2.1920251576443675\n",
      "gamma 0.01\n",
      "prev_x 2.178441640823547 \n",
      "\n",
      "previous_step_size 0.01114270508347337\n",
      "cur_x 2.203167862727841\n",
      "gamma 0.01\n",
      "prev_x 2.1920251576443675 \n",
      "\n",
      "previous_step_size 0.009092831544628588\n",
      "cur_x 2.2122606942724694\n",
      "gamma 0.01\n",
      "prev_x 2.203167862727841 \n",
      "\n",
      "previous_step_size 0.007387993490493905\n",
      "cur_x 2.2196486877629633\n",
      "gamma 0.01\n",
      "prev_x 2.2122606942724694 \n",
      "\n",
      "previous_step_size 0.005981442727957198\n",
      "cur_x 2.2256301304909205\n",
      "gamma 0.01\n",
      "prev_x 2.2196486877629633 \n",
      "\n",
      "previous_step_size 0.004828577199806894\n",
      "cur_x 2.2304587076907274\n",
      "gamma 0.01\n",
      "prev_x 2.2256301304909205 \n",
      "\n",
      "previous_step_size 0.003888674996867625\n",
      "cur_x 2.234347382687595\n",
      "gamma 0.01\n",
      "prev_x 2.2304587076907274 \n",
      "\n",
      "previous_step_size 0.0031257076070132506\n",
      "cur_x 2.2374730902946083\n",
      "gamma 0.01\n",
      "prev_x 2.234347382687595 \n",
      "\n",
      "previous_step_size 0.0025085316219675136\n",
      "cur_x 2.239981621916576\n",
      "gamma 0.01\n",
      "prev_x 2.2374730902946083 \n",
      "\n",
      "previous_step_size 0.0020106955609398014\n",
      "cur_x 2.2419923174775156\n",
      "gamma 0.01\n",
      "prev_x 2.239981621916576 \n",
      "\n",
      "previous_step_size 0.0016100341135731888\n",
      "cur_x 2.243602351591089\n",
      "gamma 0.01\n",
      "prev_x 2.2419923174775156 \n",
      "\n",
      "previous_step_size 0.0012881668940809554\n",
      "cur_x 2.2448905184851697\n",
      "gamma 0.01\n",
      "prev_x 2.243602351591089 \n",
      "\n",
      "previous_step_size 0.001029976118198661\n",
      "cur_x 2.2459204946033684\n",
      "gamma 0.01\n",
      "prev_x 2.2448905184851697 \n",
      "\n",
      "previous_step_size 0.0008231069329518448\n",
      "cur_x 2.2467436015363202\n",
      "gamma 0.01\n",
      "prev_x 2.2459204946033684 \n",
      "\n",
      "previous_step_size 0.0006575133265744881\n",
      "cur_x 2.2474011148628947\n",
      "gamma 0.01\n",
      "prev_x 2.2467436015363202 \n",
      "\n",
      "previous_step_size 0.0005250591856875708\n",
      "cur_x 2.2479261740485823\n",
      "gamma 0.01\n",
      "prev_x 2.2474011148628947 \n",
      "\n",
      "previous_step_size 0.0004191759761891234\n",
      "cur_x 2.2483453500247714\n",
      "gamma 0.01\n",
      "prev_x 2.2479261740485823 \n",
      "\n",
      "previous_step_size 0.00033457398521496984\n",
      "cur_x 2.2486799240099864\n",
      "gamma 0.01\n",
      "prev_x 2.2483453500247714 \n",
      "\n",
      "previous_step_size 0.0002670018118808848\n",
      "cur_x 2.2489469258218673\n",
      "gamma 0.01\n",
      "prev_x 2.2486799240099864 \n",
      "\n",
      "previous_step_size 0.00021304795404430976\n",
      "cur_x 2.2491599737759116\n",
      "gamma 0.01\n",
      "prev_x 2.2489469258218673 \n",
      "\n",
      "previous_step_size 0.00016997831815812603\n",
      "cur_x 2.2493299520940697\n",
      "gamma 0.01\n",
      "prev_x 2.2491599737759116 \n",
      "\n",
      "previous_step_size 0.0001356038994284603\n",
      "cur_x 2.249465555993498\n",
      "gamma 0.01\n",
      "prev_x 2.2493299520940697 \n",
      "\n",
      "previous_step_size 0.00010817350395164738\n",
      "cur_x 2.24957372949745\n",
      "gamma 0.01\n",
      "prev_x 2.249465555993498 \n",
      "\n",
      "previous_step_size 8.628707268698577e-05\n",
      "cur_x 2.249660016570137\n",
      "gamma 0.01\n",
      "prev_x 2.24957372949745 \n",
      "\n",
      "previous_step_size 6.882584014755722e-05\n",
      "cur_x 2.2497288424102844\n",
      "gamma 0.01\n",
      "prev_x 2.249660016570137 \n",
      "\n",
      "previous_step_size 5.489617795584323e-05\n",
      "cur_x 2.24978373858824\n",
      "gamma 0.01\n",
      "prev_x 2.2497288424102844 \n",
      "\n",
      "previous_step_size 4.378451786646309e-05\n",
      "cur_x 2.2498275231061067\n",
      "gamma 0.01\n",
      "prev_x 2.24978373858824 \n",
      "\n",
      "previous_step_size 3.492121652826086e-05\n",
      "cur_x 2.249862444322635\n",
      "gamma 0.01\n",
      "prev_x 2.2498275231061067 \n",
      "\n",
      "previous_step_size 2.785161888896326e-05\n",
      "cur_x 2.249890295941524\n",
      "gamma 0.01\n",
      "prev_x 2.249862444322635 \n",
      "\n",
      "previous_step_size 2.2212905597562838e-05\n",
      "cur_x 2.2499125088471215\n",
      "gamma 0.01\n",
      "prev_x 2.249890295941524 \n",
      "\n",
      "previous_step_size 1.7715580638455464e-05\n",
      "cur_x 2.24993022442776\n",
      "gamma 0.01\n",
      "prev_x 2.2499125088471215 \n",
      "\n",
      "previous_step_size 1.4128677038982573e-05\n",
      "cur_x 2.249944353104799\n",
      "gamma 0.01\n",
      "prev_x 2.24993022442776 \n",
      "\n",
      "previous_step_size 1.126793890104949e-05\n",
      "cur_x 2.2499556210437\n",
      "gamma 0.01\n",
      "prev_x 2.249944353104799 \n",
      "\n",
      "previous_step_size 8.986384145703852e-06\n",
      "cur_x 2.2499646074278457\n",
      "gamma 0.01\n",
      "prev_x 2.2499556210437 \n",
      "\n",
      "The local minimum occurs at 2.2499646074278457\n"
     ]
    }
   ],
   "source": [
    "#The gradient descent algorithm is applied to find a local minimum of the function f(x)=x4-3x3+2, with derivative #f'(x)=4x3-9x2. Here is an implementation in the Python programming language.\n",
    "\n",
    "# From calculation, it is expected that the local minimum occurs at x=9/4\n",
    "\n",
    "cur_x = 6 # The algorithm starts at x=6. As Error\n",
    "gamma = 0.01 # step size multiplier - Learning rate\n",
    "precision = 0.00001  #Difference\n",
    "previous_step_size = 1 \n",
    "max_iters = 100000 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "\n",
    "df = lambda x: 4 * x**3 - 9 * x**2\n",
    "\n",
    "while ((previous_step_size > precision) and (iters < max_iters)):\n",
    "    prev_x = cur_x\n",
    "    \n",
    "    cur_x -= gamma * df(prev_x)\n",
    "    previous_step_size = abs(cur_x - prev_x)\n",
    "    print('previous_step_size',previous_step_size)\n",
    "    iters += 1\n",
    "    print(\"cur_x\",cur_x)\n",
    "    print(\"gamma\",gamma)\n",
    "    print(\"prev_x\",prev_x, '\\n')\n",
    "    \n",
    "\n",
    "print(\"The local minimum occurs at\", cur_x)\n",
    "#The output for the above will be: ('The local minimum occurs at', 2.2499646074278457)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-38c6f2132e27>, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-38c6f2132e27>\"\u001b[1;36m, line \u001b[1;32m39\u001b[0m\n\u001b[1;33m    print \"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points))\u001b[0m\n\u001b[1;37m                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from numpy import *\n",
    "\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]\n",
    "\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]\n",
    "\n",
    "def run():\n",
    "    points = genfromtxt(\"C:\\\\Users\\\\Rebecca\\\\Documents\\\\TL Study Docs\\\\ML\\\\Dataset\\\\data.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print \"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points))\n",
    "    #print \"Running...\"\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    #print \"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
